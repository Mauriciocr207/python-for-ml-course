{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c21c8425",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "En la era de la tecnología moderna contamos con una gran cantidad de datos, tanto estructurados como no estructurados. Durante la segunda mitad del siglo XX, el aprendizaje automático surgió como una rama de la inteligencia artificial que utiliza algoritmos capaces de aprender de los datos para hacer predicciones. En lugar de que las personas definan manualmente las reglas y modelos a partir del análisis de grandes volúmenes de información, el machine learning permite capturar ese conocimiento de manera más eficiente y mejorar gradualmente el rendimiento de los modelos predictivos para tomar decisiones basadas en datos.\n",
    "\n",
    "\n",
    "<img src=\"./img/neurona.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dd9f78",
   "metadata": {},
   "source": [
    "## Tipos de Machine Learning\n",
    "\n",
    "<img src=\"./img/types-of-ml.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f391c820",
   "metadata": {},
   "source": [
    "## La definición formal de una neurona artificial\n",
    "\n",
    "Supongamos que tenemos un clasificador binario. Podemos definir una función de decisión $\\sigma(z)$, que tomará una combinación lineal de\n",
    "ciertos valores de entrada, $x$, y un vector de peso correspondiente, $w$, donde $z$ es el input neto\n",
    "$$\n",
    "    z = w_1 x_1 + w_2 x_2 + ... + w_3 x_3\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\mathbf{w} =\n",
    "    \\begin{bmatrix}\n",
    "    w_{1} \\\\\n",
    "    \\vdots \\\\\n",
    "    w_{m}\n",
    "    \\end{bmatrix}, \\quad\n",
    "\n",
    "    \\mathbf{x} =\n",
    "    \\begin{bmatrix}\n",
    "    x_{1} \\\\\n",
    "    \\vdots \\\\\n",
    "    x_{m}\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Ahora, si el valor neto de un ejemplo en particular, $x^{(i)}$, es más grande que un determinado límite, $\\theta$, entonces predecimos la clase 1 o 0 en otro caso. En el algoritmo del Perceptron, la función de decisión $\\sigma$ es una variante de la función escalón\n",
    "\n",
    "$$\n",
    "    \\sigma(z) =\n",
    "    \\begin{cases}\n",
    "    1 & \\text{if } z \\ge \\theta \\\\\n",
    "    0 & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "Simplificaremos esta implementación en un par de pasos. Primero, moveremos el límite (threshold, $\\theta$) a la izquierda de la ecuación\n",
    "\n",
    "$$\n",
    "    z \\geqslant \\theta \\\\\n",
    "    z - \\theta \\geqslant 0\n",
    "$$\n",
    "\n",
    "Definimos también la unidad de bias como $b = -\\theta$ y hacerlo parte de nuestro input neto\n",
    "\n",
    "$$\n",
    "    z = w_1 x_1 + w_2 x_2 + ... + w_3 x_3 + b = \\mathbf{w}^T x + b\n",
    "$$\n",
    "\n",
    "Entonces\n",
    "\n",
    "$$\n",
    "    \\sigma(z) =\n",
    "    \\begin{cases}\n",
    "    1 & \\text{if } z \\ge 0 \\\\\n",
    "    0 & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25111473",
   "metadata": {},
   "source": [
    "<img src=\"./img/decision-boundary.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06e552f",
   "metadata": {},
   "source": [
    "## El perceptrón y la regla de aprendizaje\n",
    "\n",
    "La idea detrás del perceptrón es simplemente imitar el comportamiento de una neurona logrando que la neurona se dispare o no. El algoritmo del perceptrón se simplifica de la siguiente manera\n",
    "\n",
    "1. Inicializa los pesos y el bias a 0 o números aleatorios pequeños\n",
    "2. Para cada ejemplo de entrenamiento, $x^{(i)}$:\n",
    "    a. Calcula el valor de salida $y^{(i)}$\n",
    "    b. Actualiza los pesos y el bias\n",
    "\n",
    "Aquí el valor de salida es la clase predecida por la función escalón y simultaneamente actualizaremos el bias y cada peso, $w_j$ en el vector $\\mathbf{w}$, que se puede escribir más formalmente como:\n",
    "\n",
    "$$\n",
    "    w_j := w_j + \\Delta w_j \\\\\n",
    "    b := b + \\Delta b\n",
    "$$\n",
    "\n",
    "Y para actualizar las \"deltas\" lo hacemos de la siguiente forma\n",
    "\n",
    "$$\n",
    "    \\Delta w_j := \\eta(y^{(i)} - \\hat{y}^{(i)}) x_j^{(i)}  \\\\\n",
    "    \\Delta b := \\eta(y^{(i)} - \\hat{y}^{(i)})\n",
    "$$\n",
    "\n",
    "| Símbolo             | Significado                                                                 |\n",
    "|---------------------|------------------------------------------------------------------------------|\n",
    "| $Δw_j$             | Cambio (ajuste) en el peso `w_j` correspondiente a la característica `j`.    |\n",
    "| $Δb$               | Cambio en el sesgo (*bias*) del perceptrón.                                  |\n",
    "| $η$                | Tasa de aprendizaje (*learning rate*), controla el tamaño del ajuste.        |\n",
    "| $y^{(i)}$            | Etiqueta real de la muestra de entrenamiento `i` (normalmente 0 o 1).        |\n",
    "| $ŷ^{(i)}$            | Predicción del perceptrón para la muestra `i` (0 o 1).                       |\n",
    "| $x_j^{(i)}$          | Valor de la característica `j` de la muestra `i`.                            |\n",
    "| $w_j$              | Peso actual de la característica `j` (se actualiza sumando `Δw_j`).          |\n",
    "| $b$                | Sesgo actual del perceptrón (se actualiza sumando `Δb`).                     |\n",
    "\n",
    "\n",
    "<img src=\"./img/algorithm.png\" width=\"800\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b720254c",
   "metadata": {},
   "source": [
    "<img src=\"./img/linear-or-nonlinear.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fb2ce5",
   "metadata": {},
   "source": [
    "Referencia:\n",
    "\n",
    "- Raschka, S., Liu, Y. y Mirjalili, V. (2022). Machine Learning with PyTorch and Scikit-Learn\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
